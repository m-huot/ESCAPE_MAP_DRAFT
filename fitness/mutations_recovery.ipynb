{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63d9ccf7",
   "metadata": {},
   "source": [
    "# Redo Marks fitness pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef498809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ESCAPE_MAP_DRAFT\\PGM\\source\\numba_utilities.py:1124: NumbaPerformanceWarning: \u001b[1m\u001b[1mnp.dot() is faster on contiguous arrays, called on (Array(float32, 2, 'F', False, aligned=True), Array(float32, 2, 'A', False, aligned=True))\u001b[0m\u001b[0m\n",
      "  dmean_v_dw = np.dot(s1.T, V)\n",
      "E:\\ESCAPE_MAP_DRAFT\\PGM\\source\\numba_utilities.py:961: NumbaPerformanceWarning: \u001b[1m\u001b[1mnp.dot() is faster on contiguous arrays, called on (Array(float32, 1, 'A', False, aligned=True), Array(float32, 2, 'A', False, aligned=True))\u001b[0m\u001b[0m\n",
      "  mean_V = np.dot(weights, V) / sum_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 29 KD vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maria\\AppData\\Roaming\\Python\\Python312\\site-packages\\Bio\\pairwise2.py:278: BiopythonDeprecationWarning: Bio.pairwise2 has been deprecated, and we intend to remove it in a future release of Biopython. As an alternative, please consider using Bio.Align.PairwiseAligner as a replacement, and contact the Biopython developers if you still need the Bio.pairwise2 module.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from utils import *\n",
    "from global_variables import *\n",
    "from escape_map import *\n",
    "pgm_path = \"PGM/\"\n",
    "sys.path.append(pgm_path + \"source/\")\n",
    "sys.path.append(pgm_path + \"utilities/\")\n",
    "import utilities, Proteins_utils, sequence_logo, plots_utils\n",
    "import rbm, RBM_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04f66bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Load ---\n",
    "df = pd.read_csv(\"rbd_dist_one_scores_gisaid.csv\")\n",
    "df=df[df['i']>=349] \n",
    "df=df[df['i']<=526]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01a99409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Unnamed: 0",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "i",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "wt",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "mut",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "bloom_ace2_binding",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "bloom_expression",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "chan_expression",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "chan_ace2_binding",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "fitness_eve",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dissimilarity_charge_hydro",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "accessibility_wcn",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "evescape",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "max_escape_experiment_bloom",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "is_escape_experiment_bloom",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "max_escape_experiment_xie",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "is_escape_experiment_xie",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "is_escape_experiment_all",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "mutation",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "counts",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "first_seen",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "6f97aaaa-3b2f-4e79-8fb6-2157c6ac1e68",
       "rows": [
        [
         "135",
         "135",
         "349",
         "S",
         "A",
         "-0.34",
         "-1.13",
         "-3.83",
         "-2.68",
         "-6.90224606",
         "-1.2222992103392023",
         "-1.4171396706900132",
         "-2.3878399928000547",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "S349A",
         "53.0",
         null
        ],
        [
         "136",
         "136",
         "349",
         "S",
         "C",
         "-0.12",
         "-2.03",
         "-1.77",
         "-0.78",
         "-7.368066379999999",
         "-1.6091905760185767",
         "-1.4171396706900132",
         "-2.563646445612577",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "S349C",
         "10.0",
         null
        ],
        [
         "137",
         "137",
         "349",
         "S",
         "F",
         "-2.03",
         "-2.5",
         "-3.46",
         "-2.75",
         "-7.207226680000001",
         "-0.559056869174561",
         "-1.4171396706900132",
         "-2.345079792328152",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "S349F",
         "16.0",
         null
        ],
        [
         "138",
         "138",
         "349",
         "S",
         "G",
         "-0.59",
         "-2.08",
         "-1.04",
         "-0.77",
         "-6.908367920000001",
         "-1.388109795630363",
         "-1.4171396706900132",
         "-2.4185688318880425",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "S349G",
         null,
         null
        ],
        [
         "139",
         "139",
         "349",
         "S",
         "I",
         "-4.25",
         "-2.69",
         "-3.35",
         "-1.95",
         "-6.50098872",
         "-0.3379760887863471",
         "-1.4171396706900132",
         "-2.1565179207249936",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "S349I",
         null,
         null
        ],
        [
         "140",
         "140",
         "349",
         "S",
         "L",
         "-1.91",
         "-2.58",
         "-3.4",
         "-2.88",
         "-6.09904174",
         "-0.7064440561000368",
         "-1.4171396706900132",
         "-2.13653944428402",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "S349L",
         null,
         null
        ],
        [
         "141",
         "141",
         "349",
         "S",
         "N",
         "-0.26",
         "-0.59",
         "-1.53",
         "-1.72",
         "-3.05394896",
         "-1.4618033890931008",
         "-1.4171396706900132",
         "-1.8599735965172088",
         "0.4698",
         "False",
         "0.4546",
         "False",
         "False",
         "S349N",
         null,
         null
        ],
        [
         "142",
         "142",
         "349",
         "S",
         "P",
         "-0.11",
         "-0.38",
         "0.5700000000000001",
         "0.51",
         "-6.455566419999999",
         "-1.811847958041106",
         "-1.4171396706900132",
         "-2.400876691349021",
         "0.8831",
         "True",
         "0.4462",
         "False",
         "True",
         "S349P",
         "545.0",
         "2021-09"
        ],
        [
         "143",
         "143",
         "349",
         "S",
         "R",
         "-0.96",
         "-2.4",
         "-2.66",
         "-2.0",
         "-7.22531744",
         "2.46736496541156",
         "-1.4171396706900132",
         "-1.965185930615908",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "S349R",
         null,
         null
        ],
        [
         "144",
         "144",
         "349",
         "S",
         "T",
         "-0.76",
         "-1.93",
         "-4.78",
         "-3.0",
         "-6.71768792",
         "-2.014505340063636",
         "-1.4171396706900132",
         "-2.4937678092485456",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "S349T",
         "6.0",
         null
        ],
        [
         "145",
         "145",
         "349",
         "S",
         "W",
         "-1.9",
         "-2.58",
         "-3.93",
         "-2.78",
         "-7.322814940000001",
         "-1.0012184299509888",
         "-1.4171396706900132",
         "-2.445226391271285",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "S349W",
         null,
         null
        ],
        [
         "146",
         "146",
         "349",
         "S",
         "Y",
         "-1.8",
         "-2.48",
         "-4.87",
         "-2.31",
         "-7.263287320000001",
         "-1.6460373727499458",
         "-1.4171396706900132",
         "-2.5458104524905854",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "S349Y",
         "8.0",
         null
        ],
        [
         "147",
         "147",
         "350",
         "V",
         "A",
         "-0.4",
         "-1.47",
         "-1.5499999999999998",
         "-0.94",
         "-8.53944702",
         "-1.6276139743842613",
         "-1.6232223214493124",
         "-3.259827526209345",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "V350A",
         "29.0",
         null
        ],
        [
         "148",
         "148",
         "350",
         "V",
         "D",
         "-4.8",
         "-2.21",
         "-6.68",
         "-5.0",
         "-8.5125306",
         "1.9515098111723943",
         "-1.6232223214493124",
         "-2.735505419887195",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "V350D",
         null,
         null
        ],
        [
         "149",
         "149",
         "350",
         "V",
         "E",
         "-4.59",
         "-2.5",
         "-5.68",
         "-4.58",
         "-8.5451294",
         "1.7672758275155491",
         "-1.6232223214493124",
         "-2.7642760692038104",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "V350E",
         null,
         null
        ],
        [
         "150",
         "150",
         "350",
         "V",
         "F",
         "-4.84",
         "-2.09",
         "-3.91",
         "-2.4",
         "-8.6748841",
         "-2.03292873842932",
         "-1.6232223214493124",
         "-3.37417259897079",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "V350F",
         "9.0",
         null
        ],
        [
         "151",
         "151",
         "350",
         "V",
         "G",
         "-1.9",
         "-2.81",
         "-3.34",
         "-2.48",
         "-8.59011826",
         "-1.4618033890931008",
         "-1.6232223214493124",
         "-3.243695767400376",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "V350G",
         null,
         null
        ],
        [
         "152",
         "152",
         "350",
         "V",
         "I",
         "-0.23",
         "-1.88",
         "-1.85",
         "-0.37",
         "-6.7841919200000005",
         "-1.8118479580411064",
         "-1.6232223214493124",
         "-2.860115611414237",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "V350I",
         "33.0",
         null
        ],
        [
         "153",
         "153",
         "350",
         "V",
         "L",
         "-1.69",
         "-2.46",
         "-2.88",
         "-2.08",
         "-8.68228158",
         "-2.143469128623426",
         "-1.6232223214493124",
         "-3.3977033031801427",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "V350L",
         null,
         null
        ],
        [
         "154",
         "154",
         "350",
         "V",
         "M",
         "-3.54",
         "-2.54",
         "0.13",
         "-0.39",
         "-8.5621031",
         "-1.6460373727499458",
         "-1.6232223214493124",
         "-3.269528126353562",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "V350M",
         null,
         null
        ],
        [
         "155",
         "155",
         "351",
         "Y",
         "C",
         "-0.6",
         "-2.01",
         "-1.01",
         "-0.66",
         "-8.4901489",
         "-2.1250457302577423",
         "-1.3560385741224117",
         "-2.8549946194645432",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "Y351C",
         null,
         null
        ],
        [
         "156",
         "156",
         "351",
         "Y",
         "D",
         "-2.72",
         "-2.86",
         "-6.4",
         "-5.0",
         "-8.43458236",
         "0.9934930961568008",
         "-1.3560385741224117",
         "-2.33900922642082",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "Y351D",
         null,
         null
        ],
        [
         "157",
         "157",
         "351",
         "Y",
         "F",
         "-0.08",
         "-0.84",
         "1.2999999999999998",
         "0.94",
         "-7.27617192",
         "-1.0749120234137266",
         "-1.3560385741224117",
         "-2.352179901639029",
         "0.08798",
         "False",
         "0.05092",
         "False",
         "False",
         "Y351F",
         "109.0",
         "2022-01"
        ],
        [
         "158",
         "158",
         "351",
         "Y",
         "H",
         "-0.27",
         "-0.6",
         "-0.19",
         "-0.5",
         "-8.448614500000001",
         "0.4039443484548973",
         "-1.3560385741224117",
         "-2.4207994243957813",
         "0.8186",
         "True",
         "0.2825",
         "False",
         "True",
         "Y351H",
         "54.0",
         null
        ],
        [
         "159",
         "159",
         "351",
         "Y",
         "N",
         "-0.77",
         "-2.48",
         "-3.58",
         "-2.94",
         "-8.29901736",
         "-0.9459482348539352",
         "-1.3560385741224117",
         "-2.58800572889782",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "Y351N",
         null,
         null
        ],
        [
         "160",
         "160",
         "351",
         "Y",
         "S",
         "-0.71",
         "-1.73",
         "-3.26",
         "-2.51",
         "-8.51289052",
         "-1.6460373727499458",
         "-1.3560385741224117",
         "-2.770407752290509",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "Y351S",
         null,
         null
        ],
        [
         "161",
         "161",
         "352",
         "A",
         "D",
         "-0.03",
         "-0.45",
         "-1.27",
         "-0.93",
         "-7.47478034",
         "1.4172312585675442",
         "-1.227414287944617",
         "-1.8720626353040857",
         "0.7057",
         "True",
         "0.4982",
         "False",
         "True",
         "A352D",
         "39.0",
         null
        ],
        [
         "162",
         "162",
         "352",
         "A",
         "E",
         "-0.03",
         "-0.2",
         "0.06",
         "0.42",
         "-8.298297199999999",
         "1.2329972749106992",
         "-1.227414287944617",
         "-2.1044055693783275",
         "0.8812",
         "True",
         "0.7312",
         "False",
         "True",
         "A352E",
         null,
         null
        ],
        [
         "163",
         "163",
         "352",
         "A",
         "G",
         "-0.19",
         "-0.98",
         "-1.77",
         "-1.07",
         "-8.41944586",
         "-1.996081941697951",
         "-1.227414287944617",
         "-2.642555670873122",
         "0.2058",
         "False",
         "0.3761",
         "False",
         "False",
         "A352G",
         "23.0",
         null
        ],
        [
         "164",
         "164",
         "352",
         "A",
         "P",
         "-3.21",
         "-2.42",
         "-2.21",
         "-1.14",
         "-8.3124206",
         "-1.5723437792872077",
         "-1.227414287944617",
         "-2.533944946307982",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "A352P",
         null,
         null
        ],
        [
         "165",
         "165",
         "352",
         "A",
         "S",
         "-0.06",
         "-0.26",
         "1.61",
         "1.13",
         "-7.714447100000001",
         "-1.2222992103392023",
         "-1.227414287944617",
         "-2.315648058837843",
         "0.05001",
         "False",
         "0.1706",
         "False",
         "False",
         "A352S",
         "2316.0",
         "2021-01"
        ],
        [
         "166",
         "166",
         "352",
         "A",
         "T",
         "-0.27",
         "-1.24",
         "0.21",
         "0.38",
         "-8.24923106",
         "-1.3696863972646784",
         "-1.227414287944617",
         "-2.4802769317055384",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "A352T",
         "77.0",
         null
        ],
        [
         "167",
         "167",
         "352",
         "A",
         "V",
         "-0.52",
         "-1.79",
         "-4.55",
         "-3.68",
         "-6.442120320000001",
         "-1.6276139743842613",
         "-1.227414287944617",
         "-2.101070467518097",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "A352V",
         "625.0",
         "2021-08"
        ],
        [
         "168",
         "168",
         "353",
         "W",
         "C",
         "-1.22",
         "-2.53",
         "-2.14",
         "-2.18",
         "-9.44750374",
         "-1.5539203809215234",
         "-1.6003755826050836",
         "-3.462650614673413",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "W353C",
         "17.0",
         null
        ],
        [
         "169",
         "169",
         "353",
         "W",
         "G",
         "-2.17",
         "-2.27",
         "-2.51",
         "-1.65",
         "-9.091650400000002",
         "-1.7750011613097372",
         "-1.6003755826050836",
         "-3.3968541491497555",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "W353G",
         null,
         "2022-03"
        ],
        [
         "170",
         "170",
         "353",
         "W",
         "L",
         "-2.15",
         "-2.62",
         "-1.8",
         "-1.05",
         "-9.2524536",
         "-1.8671181531381595",
         "-1.6003755826050836",
         "-3.461980718283091",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "W353L",
         "13.0",
         "2022-04"
        ],
        [
         "171",
         "171",
         "353",
         "W",
         "R",
         "-1.62",
         "-2.52",
         "-1.7",
         "-1.42",
         "-9.13148208",
         "3.628039062449683",
         "-1.6003755826050836",
         "-2.7153746996257127",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "W353R",
         "18.0",
         null
        ],
        [
         "172",
         "172",
         "353",
         "W",
         "S",
         "-0.98",
         "-2.02",
         "-5.58",
         "-4.82",
         "-9.2277284",
         "-1.0012184299509888",
         "-1.6003755826050836",
         "-3.298722362265901",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "W353S",
         "10.0",
         null
        ],
        [
         "173",
         "173",
         "354",
         "N",
         "D",
         "-0.04",
         "0.13",
         "2.01",
         "1.63",
         "-2.8103088200000004",
         "-0.2224511959783752",
         "-1.4466051992800253",
         "-1.6822943966997403",
         "0.1033",
         "False",
         "0.1872",
         "False",
         "False",
         "N354D",
         "1284.0",
         "2021-02"
        ],
        [
         "174",
         "174",
         "354",
         "N",
         "H",
         "-0.07",
         "-0.44",
         "1.88",
         "0.99",
         "-2.92830198",
         "0.0723231778725767",
         "-1.4466051992800253",
         "-1.6476923002574564",
         "0.06028",
         "False",
         "0.00818",
         "False",
         "False",
         "N354H",
         "322.0",
         "2021-10"
        ],
        [
         "175",
         "175",
         "354",
         "N",
         "I",
         "-0.39",
         "-1.1",
         "-2.0",
         "-1.33",
         "-2.8149414",
         "0.3621130491096634",
         "-1.4466051992800253",
         "-1.5971106994175115",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "N354I",
         "66.0",
         null
        ],
        [
         "176",
         "176",
         "354",
         "N",
         "K",
         "0.03",
         "0.21",
         "1.73",
         "0.7",
         "-0.0848754892",
         "0.4776379419176355",
         "-1.4466051992800253",
         "-1.439005707694096",
         "0.0571",
         "False",
         "0.6128",
         "False",
         "False",
         "N354K",
         "3722.0",
         "2021-03"
        ],
        [
         "177",
         "177",
         "354",
         "N",
         "S",
         "0.04",
         "0.17",
         "1.05",
         "0.65",
         "-1.6783996600000002",
         "-1.4618033890931008",
         "-1.4466051992800253",
         "-1.814183051095116",
         "0.04863",
         "False",
         "0.169",
         "False",
         "False",
         "N354S",
         "640.0",
         "2021-07"
        ],
        [
         "178",
         "178",
         "354",
         "N",
         "T",
         "0.02",
         "0.02",
         "1.22",
         "0.79",
         "-2.90637206",
         "-1.314416202167625",
         "-1.4466051992800253",
         "-1.87046615998922",
         "0.05233",
         "False",
         "0.8086",
         "False",
         "False",
         "N354T",
         "1869.0",
         "2021-08"
        ],
        [
         "179",
         "179",
         "354",
         "N",
         "Y",
         "-0.38",
         "-1.22",
         "-0.76",
         "-0.67",
         "-3.07973634",
         "-0.9459482348539352",
         "-1.4466051992800253",
         "-1.8211040630827464",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "N354Y",
         "46.0",
         null
        ],
        [
         "180",
         "180",
         "355",
         "R",
         "C",
         "-1.88",
         "-1.4",
         "-4.08",
         "-3.03",
         "-9.6218442",
         "3.0200669163820946",
         "-1.4648448800975744",
         "-2.650435408853428",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "R355C",
         null,
         null
        ],
        [
         "181",
         "181",
         "355",
         "R",
         "G",
         "-2.15",
         "-2.24",
         "-1.52",
         "-1.33",
         "-9.7600648",
         "3.241147696770309",
         "-1.4648448800975744",
         "-2.675249507199072",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "R355G",
         null,
         "2021-08"
        ],
        [
         "182",
         "182",
         "355",
         "R",
         "H",
         "-0.38",
         "-1.46",
         "-1.7",
         "-0.99",
         "-9.602887",
         "0.4173832442067166",
         "-1.4648448800975744",
         "-2.932108763589722",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "R355H",
         null,
         null
        ],
        [
         "183",
         "183",
         "355",
         "R",
         "I",
         "-3.34",
         "-2.53",
         "-3.33",
         "-3.31",
         "-9.68645",
         "4.291281403614324",
         "-1.4648448800975744",
         "-2.57718924498009",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "R355I",
         null,
         null
        ],
        [
         "184",
         "184",
         "355",
         "R",
         "K",
         "-1.08",
         "-2.21",
         "-1.05",
         "-0.37",
         "-9.5220034",
         "-0.8722546413911973",
         "-1.4648448800975744",
         "-3.104617067473264",
         "0.0",
         "False",
         "0.0",
         "False",
         "False",
         "R355K",
         "6.0",
         null
        ]
       ],
       "shape": {
        "columns": 20,
        "rows": 1401
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>i</th>\n",
       "      <th>wt</th>\n",
       "      <th>mut</th>\n",
       "      <th>bloom_ace2_binding</th>\n",
       "      <th>bloom_expression</th>\n",
       "      <th>chan_expression</th>\n",
       "      <th>chan_ace2_binding</th>\n",
       "      <th>fitness_eve</th>\n",
       "      <th>dissimilarity_charge_hydro</th>\n",
       "      <th>accessibility_wcn</th>\n",
       "      <th>evescape</th>\n",
       "      <th>max_escape_experiment_bloom</th>\n",
       "      <th>is_escape_experiment_bloom</th>\n",
       "      <th>max_escape_experiment_xie</th>\n",
       "      <th>is_escape_experiment_xie</th>\n",
       "      <th>is_escape_experiment_all</th>\n",
       "      <th>mutation</th>\n",
       "      <th>counts</th>\n",
       "      <th>first_seen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>135</td>\n",
       "      <td>349</td>\n",
       "      <td>S</td>\n",
       "      <td>A</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>-3.83</td>\n",
       "      <td>-2.68</td>\n",
       "      <td>-6.902246</td>\n",
       "      <td>-1.222299</td>\n",
       "      <td>-1.417140</td>\n",
       "      <td>-2.387840</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>S349A</td>\n",
       "      <td>53.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>136</td>\n",
       "      <td>349</td>\n",
       "      <td>S</td>\n",
       "      <td>C</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-2.03</td>\n",
       "      <td>-1.77</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>-7.368066</td>\n",
       "      <td>-1.609191</td>\n",
       "      <td>-1.417140</td>\n",
       "      <td>-2.563646</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>S349C</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>137</td>\n",
       "      <td>349</td>\n",
       "      <td>S</td>\n",
       "      <td>F</td>\n",
       "      <td>-2.03</td>\n",
       "      <td>-2.50</td>\n",
       "      <td>-3.46</td>\n",
       "      <td>-2.75</td>\n",
       "      <td>-7.207227</td>\n",
       "      <td>-0.559057</td>\n",
       "      <td>-1.417140</td>\n",
       "      <td>-2.345080</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>S349F</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>138</td>\n",
       "      <td>349</td>\n",
       "      <td>S</td>\n",
       "      <td>G</td>\n",
       "      <td>-0.59</td>\n",
       "      <td>-2.08</td>\n",
       "      <td>-1.04</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>-6.908368</td>\n",
       "      <td>-1.388110</td>\n",
       "      <td>-1.417140</td>\n",
       "      <td>-2.418569</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>S349G</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>139</td>\n",
       "      <td>349</td>\n",
       "      <td>S</td>\n",
       "      <td>I</td>\n",
       "      <td>-4.25</td>\n",
       "      <td>-2.69</td>\n",
       "      <td>-3.35</td>\n",
       "      <td>-1.95</td>\n",
       "      <td>-6.500989</td>\n",
       "      <td>-0.337976</td>\n",
       "      <td>-1.417140</td>\n",
       "      <td>-2.156518</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>S349I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1531</th>\n",
       "      <td>1531</td>\n",
       "      <td>526</td>\n",
       "      <td>G</td>\n",
       "      <td>E</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-8.738202</td>\n",
       "      <td>1.067187</td>\n",
       "      <td>-1.885148</td>\n",
       "      <td>-3.535322</td>\n",
       "      <td>0.07851</td>\n",
       "      <td>False</td>\n",
       "      <td>0.010790</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>G526E</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1532</th>\n",
       "      <td>1532</td>\n",
       "      <td>526</td>\n",
       "      <td>G</td>\n",
       "      <td>R</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-8.578857</td>\n",
       "      <td>3.241148</td>\n",
       "      <td>-1.885148</td>\n",
       "      <td>-3.269983</td>\n",
       "      <td>0.04760</td>\n",
       "      <td>False</td>\n",
       "      <td>0.008869</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>G526R</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1533</th>\n",
       "      <td>1533</td>\n",
       "      <td>526</td>\n",
       "      <td>G</td>\n",
       "      <td>S</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-8.599188</td>\n",
       "      <td>-1.388110</td>\n",
       "      <td>-1.885148</td>\n",
       "      <td>-3.868234</td>\n",
       "      <td>0.05173</td>\n",
       "      <td>False</td>\n",
       "      <td>0.008869</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>G526S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1534</th>\n",
       "      <td>1534</td>\n",
       "      <td>526</td>\n",
       "      <td>G</td>\n",
       "      <td>V</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-8.664563</td>\n",
       "      <td>-1.461803</td>\n",
       "      <td>-1.885148</td>\n",
       "      <td>-3.899844</td>\n",
       "      <td>0.05142</td>\n",
       "      <td>False</td>\n",
       "      <td>0.008869</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>G526V</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1535</th>\n",
       "      <td>1535</td>\n",
       "      <td>526</td>\n",
       "      <td>G</td>\n",
       "      <td>W</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-8.828717</td>\n",
       "      <td>-1.775001</td>\n",
       "      <td>-1.885148</td>\n",
       "      <td>-4.004163</td>\n",
       "      <td>0.05658</td>\n",
       "      <td>False</td>\n",
       "      <td>0.009626</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>G526W</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1401 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0    i wt mut  bloom_ace2_binding  bloom_expression  \\\n",
       "135          135  349  S   A               -0.34             -1.13   \n",
       "136          136  349  S   C               -0.12             -2.03   \n",
       "137          137  349  S   F               -2.03             -2.50   \n",
       "138          138  349  S   G               -0.59             -2.08   \n",
       "139          139  349  S   I               -4.25             -2.69   \n",
       "...          ...  ... ..  ..                 ...               ...   \n",
       "1531        1531  526  G   E               -0.01             -0.13   \n",
       "1532        1532  526  G   R               -0.08             -0.30   \n",
       "1533        1533  526  G   S               -0.02             -0.14   \n",
       "1534        1534  526  G   V               -0.04             -0.12   \n",
       "1535        1535  526  G   W               -0.08             -0.44   \n",
       "\n",
       "      chan_expression  chan_ace2_binding  fitness_eve  \\\n",
       "135             -3.83              -2.68    -6.902246   \n",
       "136             -1.77              -0.78    -7.368066   \n",
       "137             -3.46              -2.75    -7.207227   \n",
       "138             -1.04              -0.77    -6.908368   \n",
       "139             -3.35              -1.95    -6.500989   \n",
       "...               ...                ...          ...   \n",
       "1531              NaN                NaN    -8.738202   \n",
       "1532              NaN                NaN    -8.578857   \n",
       "1533              NaN                NaN    -8.599188   \n",
       "1534              NaN                NaN    -8.664563   \n",
       "1535              NaN                NaN    -8.828717   \n",
       "\n",
       "      dissimilarity_charge_hydro  accessibility_wcn  evescape  \\\n",
       "135                    -1.222299          -1.417140 -2.387840   \n",
       "136                    -1.609191          -1.417140 -2.563646   \n",
       "137                    -0.559057          -1.417140 -2.345080   \n",
       "138                    -1.388110          -1.417140 -2.418569   \n",
       "139                    -0.337976          -1.417140 -2.156518   \n",
       "...                          ...                ...       ...   \n",
       "1531                    1.067187          -1.885148 -3.535322   \n",
       "1532                    3.241148          -1.885148 -3.269983   \n",
       "1533                   -1.388110          -1.885148 -3.868234   \n",
       "1534                   -1.461803          -1.885148 -3.899844   \n",
       "1535                   -1.775001          -1.885148 -4.004163   \n",
       "\n",
       "      max_escape_experiment_bloom  is_escape_experiment_bloom  \\\n",
       "135                       0.00000                       False   \n",
       "136                       0.00000                       False   \n",
       "137                       0.00000                       False   \n",
       "138                       0.00000                       False   \n",
       "139                       0.00000                       False   \n",
       "...                           ...                         ...   \n",
       "1531                      0.07851                       False   \n",
       "1532                      0.04760                       False   \n",
       "1533                      0.05173                       False   \n",
       "1534                      0.05142                       False   \n",
       "1535                      0.05658                       False   \n",
       "\n",
       "      max_escape_experiment_xie  is_escape_experiment_xie  \\\n",
       "135                    0.000000                     False   \n",
       "136                    0.000000                     False   \n",
       "137                    0.000000                     False   \n",
       "138                    0.000000                     False   \n",
       "139                    0.000000                     False   \n",
       "...                         ...                       ...   \n",
       "1531                   0.010790                     False   \n",
       "1532                   0.008869                     False   \n",
       "1533                   0.008869                     False   \n",
       "1534                   0.008869                     False   \n",
       "1535                   0.009626                     False   \n",
       "\n",
       "      is_escape_experiment_all mutation  counts first_seen  \n",
       "135                      False    S349A    53.0        NaN  \n",
       "136                      False    S349C    10.0        NaN  \n",
       "137                      False    S349F    16.0        NaN  \n",
       "138                      False    S349G     NaN        NaN  \n",
       "139                      False    S349I     NaN        NaN  \n",
       "...                        ...      ...     ...        ...  \n",
       "1531                     False    G526E     NaN        NaN  \n",
       "1532                     False    G526R     6.0        NaN  \n",
       "1533                     False    G526S     NaN        NaN  \n",
       "1534                     False    G526V     5.0        NaN  \n",
       "1535                     False    G526W     NaN        NaN  \n",
       "\n",
       "[1401 rows x 20 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f39af68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25767309064953603\n"
     ]
    }
   ],
   "source": [
    "Y_ALL = (df[\"first_seen\"].notna()).astype(int).to_numpy()\n",
    "scores_baseline=df['evescape'].to_numpy()\n",
    "#average of y\n",
    "print(Y_ALL.sum()/len(Y_ALL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8aa4a38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SVYAWNRKRISNCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIADYNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPCNGVEGFNCYFPLQSYGFQPTNGVGYQPYRVVVLSFELLHAPATVCG'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6825a109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence(WT, site,\tmutation):\n",
    "    #site is expected to be 349 instead of 0\n",
    "    site=site-349\n",
    "    return WT[:site]+mutation+WT[site+1:]\n",
    "\n",
    "#example:\n",
    "\n",
    "df['sequence']=df.apply(lambda x: get_sequence(WT, x['i'],  x['mut']), axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e934ac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('single_variants.fasta', 'w') as f:\n",
    "    for index, row in df.iterrows():\n",
    "        f.write('>' + str(index) + '\\n')\n",
    "        f.write(row['sequence'] + '\\n')\n",
    "seqs=Proteins_utils.load_FASTA('single_variants.fasta', drop_duplicates=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acfd97e",
   "metadata": {},
   "source": [
    "# EscapeMap scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44e6ae37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.6440163541444706\n"
     ]
    }
   ],
   "source": [
    "raw_conc = np.full(len(KD_VECTORS), -8.0, dtype=np.float64)  # antibodies\n",
    "model = EscapeMap(\n",
    "    rbm=RBM,\n",
    "    kd_vectors=KD_VECTORS,\n",
    "    ace2_vector=ACE2_KD_VECTOR,\n",
    "    raw_concentrations=raw_conc,\n",
    "    raw_ace2=-9,      # ACE2 concentration in log10 space\n",
    "    raw_beta=-1,\n",
    ")\n",
    "scores=-model(seqs)\n",
    "#auc to compare score and y\n",
    "auc = roc_auc_score(Y_ALL, scores)\n",
    "print(\"AUC:\", auc)\n",
    "#params are the individual concentrations for each Ab (dont put -12 everywhere), raw_ace2, raw_beta\n",
    "# i want to find best parameters to maximize predictive power=auc, but could be something else\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58c7fe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted = [\"S309\", \"LY-CoV555\", \"REGN10987\", \"REGN10933\"]\n",
    "\n",
    "# strict: fail if any key is missing\n",
    "for k in wanted:\n",
    "    assert k in KD_VECTORS, f\"Missing key: {k}\"\n",
    "kd_vector_small = {k: KD_VECTORS[k] for k in wanted}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec40c448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Evescape: 0.6318000213083315\n"
     ]
    }
   ],
   "source": [
    "auc = roc_auc_score(Y_ALL, scores_baseline)\n",
    "print(\"AUC Evescape:\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0cf7054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores shape: tensor([578.0441, 574.5410, 574.5409,  ..., 576.8531, 576.6968, 572.7727],\n",
      "       dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "AUC: 0.6440163541444706\n"
     ]
    }
   ],
   "source": [
    "from escape_map_torch import EscapeMapTorch\n",
    "raw_conc = np.full(len(KD_VECTORS), -8.0, dtype=np.float64)  # antibodies\n",
    "model = EscapeMapTorch(\n",
    "    rbm=RBM,\n",
    "    kd_vectors=KD_VECTORS,\n",
    "    ace2_vector=ACE2_KD_VECTOR,\n",
    "    raw_concentrations=raw_conc,\n",
    "    raw_ace2=-9,      # ACE2 concentration in log10 space\n",
    "    raw_beta=-1,\n",
    ")\n",
    "scores=-model(seqs)\n",
    "print('scores shape:', scores)\n",
    "#auc to compare score and y\n",
    "\n",
    "auc = roc_auc_score(Y_ALL, scores.detach().numpy())\n",
    "print(\"AUC:\", auc)\n",
    "#params are the individual concentrations for each Ab (dont put -12 everywhere), raw_ace2, raw_beta\n",
    "# i want to find best parameters to maximize predictive power=auc, but could be something else\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31aed7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 001 | loss 0.5743 | AUC 0.6825\n",
      "epoch 002 | loss 0.6054 | AUC 0.6836\n",
      "epoch 003 | loss 0.5646 | AUC 0.6886\n",
      "epoch 004 | loss 0.6122 | AUC 0.6967\n",
      "epoch 005 | loss 0.5720 | AUC 0.7044\n",
      "epoch 006 | loss 0.5983 | AUC 0.7088\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mE:\\ESCAPE_MAP_DRAFT\\escape_map_torch.py:70\u001b[0m, in \u001b[0;36mEscapeMapTorch._get_Kd_batch_numpy\u001b[1;34m(self, seqs_np, vectors, log10)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 70\u001b[0m     out \u001b[38;5;241m=\u001b[39m get_Kd(seqs_np, vectors, log10\u001b[38;5;241m=\u001b[39mlog10)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[1;32me:\\ESCAPE_MAP_DRAFT\\fitness\\..\\utils.py:65\u001b[0m, in \u001b[0;36mget_Kd\u001b[1;34m(s, kd_vectors, log10)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03mCompute Kd for a given sequence s.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03ms: array of ints (sequence positions encoded as 0..20)\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03mkd_vectors: dict of q vectors\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03mlog10: if True, return log10(Kd) instead of Kd\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m one_hot_s \u001b[38;5;241m=\u001b[39m one_hot_encode_concat(s)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [1, 3738]\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m log10:\n",
      "File \u001b[1;32me:\\ESCAPE_MAP_DRAFT\\fitness\\..\\utils.py:54\u001b[0m, in \u001b[0;36mone_hot_encode_concat\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     53\u001b[0m one_hot_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(s), num_categories), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m---> 54\u001b[0m one_hot_matrix[np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(s)), s] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m one_hot_matrix\u001b[38;5;241m.\u001b[39mflatten()\n",
      "\u001b[1;31mIndexError\u001b[0m: shape mismatch: indexing arrays could not be broadcast together with shapes (256,) (256,178) ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 72\u001b[0m\n\u001b[0;32m     69\u001b[0m yb \u001b[38;5;241m=\u001b[39m Y[bidx]\n\u001b[0;32m     71\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 72\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mmodel(xb)  \u001b[38;5;66;03m# higher score = more positive\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pairwise:\n\u001b[0;32m     74\u001b[0m     loss \u001b[38;5;241m=\u001b[39m pairwise_logistic_loss(scores, yb, margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mE:\\ESCAPE_MAP_DRAFT\\escape_map_torch.py:97\u001b[0m, in \u001b[0;36mEscapeMapTorch.forward\u001b[1;34m(self, s)\u001b[0m\n\u001b[0;32m     93\u001b[0m kds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(kds_np, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# ----- ACE2 Kd -----\u001b[39;00m\n\u001b[0;32m     96\u001b[0m kdace2_np \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m---> 97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_Kd_batch_numpy(seqs_np, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mace2_vector, log10\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m*\u001b[39m ln10\n\u001b[0;32m     98\u001b[0m )\n\u001b[0;32m     99\u001b[0m kdace2_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(kdace2_np)  \u001b[38;5;66;03m# () or (N,)\u001b[39;00m\n\u001b[0;32m    100\u001b[0m kdace2_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(kdace2_np, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m15.0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5.0\u001b[39m)\n",
      "File \u001b[1;32mE:\\ESCAPE_MAP_DRAFT\\escape_map_torch.py:72\u001b[0m, in \u001b[0;36mEscapeMapTorch._get_Kd_batch_numpy\u001b[1;34m(self, seqs_np, vectors, log10)\u001b[0m\n\u001b[0;32m     70\u001b[0m     out \u001b[38;5;241m=\u001b[39m get_Kd(seqs_np, vectors, log10\u001b[38;5;241m=\u001b[39mlog10)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m---> 72\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([get_Kd(x, vectors, log10\u001b[38;5;241m=\u001b[39mlog10) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m seqs_np], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(out)\n",
      "File \u001b[1;32me:\\ESCAPE_MAP_DRAFT\\fitness\\..\\utils.py:65\u001b[0m, in \u001b[0;36mget_Kd\u001b[1;34m(s, kd_vectors, log10)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_Kd\u001b[39m(s, kd_vectors, log10\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    Compute Kd for a given sequence s.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m    s: array of ints (sequence positions encoded as 0..20)\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    kd_vectors: dict of q vectors\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m    log10: if True, return log10(Kd) instead of Kd\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m     one_hot_s \u001b[38;5;241m=\u001b[39m one_hot_encode_concat(s)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [1, 3738]\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m log10:\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m get_deltaG(one_hot_s, kd_vectors) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[1;32me:\\ESCAPE_MAP_DRAFT\\fitness\\..\\utils.py:55\u001b[0m, in \u001b[0;36mone_hot_encode_concat\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     53\u001b[0m one_hot_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(s), num_categories), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m     54\u001b[0m one_hot_matrix[np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(s)), s] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m one_hot_matrix\u001b[38;5;241m.\u001b[39mflatten()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from math import ceil\n",
    "# assumes EscapeMapTorch is defined as above, and KD_VECTORS, ACE2_KD_VECTOR, RBM, WT_SEQ exist\n",
    "\n",
    "# ----- loss: pairwise logistic (AUC surrogate) -----\n",
    "def pairwise_logistic_loss(scores: torch.Tensor, y: torch.Tensor, margin: float = 0.0, max_pairs: int = 200_000):\n",
    "    # scores: [B], y: [B] in {0,1}\n",
    "    pos = scores[y == 1]\n",
    "    neg = scores[y == 0]\n",
    "    if pos.numel() == 0 or neg.numel() == 0:\n",
    "        return None  # skip batch with single class\n",
    "    # all pairwise diffs\n",
    "    # sample pairs if too many\n",
    "    num_pairs = pos.numel() * neg.numel()\n",
    "    if num_pairs > max_pairs:\n",
    "        k = max_pairs\n",
    "        ip = torch.randint(0, pos.numel(), (k,), device=scores.device)\n",
    "        ineg = torch.randint(0, neg.numel(), (k,), device=scores.device)\n",
    "        diff = pos[ip] - neg[ineg]\n",
    "    else:\n",
    "        diff = pos[:, None] - neg[None, :]\n",
    "    # loss = log(1 + exp(-(diff - margin))) = softplus(-(diff - margin))\n",
    "    return F.softplus(-(diff - margin)).mean()\n",
    "\n",
    "# ----- optional: BCE loss (logistic classification) -----\n",
    "bce = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# ===== data =====\n",
    "# seqs: np.ndarray [N, L] int; y: np.ndarray [N] in {0,1}\n",
    "# provide your arrays:\n",
    "# seqs = ...\n",
    "# y = ...\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float64  # to match original\n",
    "\n",
    "model = EscapeMapTorch(\n",
    "    rbm=RBM, kd_vectors=KD_VECTORS, ace2_vector=ACE2_KD_VECTOR,\n",
    "    raw_concentrations=None, raw_ace2=None, raw_beta=None,\n",
    "    total_beta=1.0, device=device, dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "# optimize only the three parameters\n",
    "opt = torch.optim.Adam(\n",
    "    [model.raw_concentrations, model.raw_beta, model.raw_ace2],\n",
    "    lr=2e-1, weight_decay=1e-5\n",
    ")\n",
    "\n",
    "# ===== training =====\n",
    "X = torch.as_tensor(seqs, dtype=torch.long, device=device)\n",
    "Y = torch.as_tensor(Y_ALL, dtype=dtype, device=device)\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 256\n",
    "use_pairwise = True  # set False to train with BCE\n",
    "\n",
    "N = X.size(0)\n",
    "idx_all = torch.arange(N, device=device)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # shuffle\n",
    "    perm = idx_all[torch.randperm(N, device=device)]\n",
    "    for bi in range(ceil(N / batch_size)):\n",
    "        bidx = perm[bi * batch_size : (bi + 1) * batch_size]\n",
    "        xb = X[bidx]\n",
    "        yb = Y[bidx]\n",
    "\n",
    "        opt.zero_grad()\n",
    "        scores = -model(xb)  # higher score = more positive\n",
    "        if use_pairwise:\n",
    "            loss = pairwise_logistic_loss(scores, yb, margin=0.0)\n",
    "            if loss is None:\n",
    "                # fallback to BCE if only one class in batch\n",
    "                loss = bce(scores, yb)\n",
    "        else:\n",
    "            loss = bce(scores, yb)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            [model.raw_concentrations, model.raw_beta, model.raw_ace2], max_norm=10.0\n",
    "        )\n",
    "        opt.step()\n",
    "\n",
    "    # evaluate AUC on full set\n",
    "    with torch.no_grad():\n",
    "        full_scores = -model(X).detach().cpu().numpy()\n",
    "    auc = roc_auc_score(Y_ALL, full_scores)\n",
    "    print(f\"epoch {epoch:03d} | loss {float(loss):.4f} | AUC {auc:.4f}\")\n",
    "\n",
    "# ===== final scores & AUC =====\n",
    "with torch.no_grad():\n",
    "    scores = -model(X).cpu().numpy()\n",
    "final_auc = roc_auc_score(y, scores)\n",
    "print(\"Final AUC:\", final_auc)\n",
    "\n",
    "# access learned params\n",
    "rc = model.raw_concentrations.detach().cpu().numpy()\n",
    "rb = float(model.raw_beta.detach().cpu())\n",
    "ra = float(model.raw_ace2.detach().cpu())\n",
    "print(\"raw_concentrations shape:\", rc)\n",
    "print(\"raw_beta:\", rb, \"raw_ace2:\", ra)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67819ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ceafe3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2020-01-01 00:00:00 to 2022-08-01 00:00:00\n",
      "Last date of first_seen: 2022-08-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1) parse the date column\n",
    "col = \"first seen\" if \"first seen\" in df.columns else \"first_seen\"\n",
    "df[\"first_seen_dt\"] = pd.to_datetime(df[col], format=\"%Y-%m\", errors=\"coerce\")\n",
    "\n",
    "# 2) 6-month periods starting 2021-01-01\n",
    "start = pd.Timestamp(\"2020-01-01\")\n",
    "end = df[\"first_seen_dt\"].max(skipna=True)\n",
    "print(\"Date range:\", start, \"to\", end)\n",
    "check_starts = pd.date_range(start, end, freq=\"12MS\")              # 2021-01, 2021-07, ...\n",
    "check_ends = check_starts + pd.DateOffset(months=12)\n",
    "\n",
    "# 3) 1 if first_seen is within the 6-month window [start, end)\n",
    "dates = df[\"first_seen_dt\"].values[:, None]                       # (N,1) datetime64[ns]\n",
    "starts = check_starts.values[None, :]                             # (1,M)\n",
    "ends = check_ends.values[None, :]                                 # (1,M)\n",
    "valid = (~pd.isna(df[\"first_seen_dt\"]).values)[:, None]           # (N,1) bool\n",
    "y_mat = (valid & (dates >= starts) & (dates < ends)).astype(int)  # (N,M)\n",
    "\n",
    "# 4) attach as columns: y_YYYY-MM (period start)\n",
    "y_df = pd.DataFrame(\n",
    "    y_mat,\n",
    "    index=df.index,\n",
    "    columns=[f\"y_{d.strftime('%Y-%m')}\" for d in check_starts]\n",
    ")\n",
    "print(\"Last date of first_seen:\", df[\"first_seen_dt\"].max())\n",
    "df_out = pd.concat([df, y_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0d24b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from math import ceil\n",
    "\n",
    "# # pairwise AUC surrogate\n",
    "# def pairwise_logistic_loss(scores: torch.Tensor, y: torch.Tensor, margin: float = 0.0, max_pairs: int = 200_000):\n",
    "#     pos = scores[y == 1]\n",
    "#     neg = scores[y == 0]\n",
    "#     if pos.numel() == 0 or neg.numel() == 0:\n",
    "#         return None\n",
    "#     num_pairs = pos.numel() * neg.numel()\n",
    "#     if num_pairs > max_pairs:\n",
    "#         k = max_pairs\n",
    "#         ip = torch.randint(0, pos.numel(), (k,), device=scores.device)\n",
    "#         ineg = torch.randint(0, neg.numel(), (k,), device=scores.device)\n",
    "#         diff = pos[ip] - neg[ineg]\n",
    "#     else:\n",
    "#         diff = pos[:, None] - neg[None, :]\n",
    "#     return F.softplus(-(diff - margin)).mean()\n",
    "\n",
    "# bce = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# def optimize_for_check_dates(\n",
    "#     df, seqs,\n",
    "#     check_prefix=\"y_\", lr=2e-1, weight_decay=1e-5,\n",
    "#     epochs=30, batch_size=1024, use_pairwise=True,\n",
    "#     device=None, dtype=torch.float64,\n",
    "#     params_csv=\"fitted_params.csv\", scores_csv=\"scores_by_date.csv\"\n",
    "# ):\n",
    "#     device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     check_cols = sorted([c for c in df.columns if c.startswith(check_prefix)])\n",
    "#     ab_names = list(kd_vector_small.keys())\n",
    "\n",
    "#     X = torch.as_tensor(seqs, dtype=torch.long, device=device)\n",
    "\n",
    "#     param_rows = []\n",
    "#     score_rows = []\n",
    "\n",
    "#     for col in check_cols:\n",
    "#         y = df[col].to_numpy().astype(int)\n",
    "#         if y.sum() == 0 or y.sum() == y.size:\n",
    "#             print(f\"{col}: skipped (single class)\")\n",
    "#             continue\n",
    "\n",
    "#         Y = torch.as_tensor(y, dtype=dtype, device=device)\n",
    "\n",
    "#         model = EscapeMapTorch(\n",
    "#             rbm=RBM, kd_vectors=kd_vector_small, ace2_vector=ACE2_KD_VECTOR,\n",
    "#             raw_concentrations=None, raw_ace2=None, raw_beta=None,\n",
    "#             total_beta=1.0, device=device, dtype=dtype\n",
    "#         ).to(device)\n",
    "\n",
    "#         opt = torch.optim.Adam(\n",
    "#             [model.raw_concentrations, model.raw_beta, model.raw_ace2],\n",
    "#             lr=lr, weight_decay=weight_decay\n",
    "#         )\n",
    "\n",
    "#         N = X.size(0)\n",
    "#         idx_all = torch.arange(N, device=device)\n",
    "\n",
    "#         for _ in range(epochs):\n",
    "#             perm = idx_all[torch.randperm(N, device=device)]\n",
    "#             for bi in range(ceil(N / batch_size)):\n",
    "#                 bidx = perm[bi * batch_size : (bi + 1) * batch_size]\n",
    "#                 xb = X[bidx]\n",
    "#                 yb = Y[bidx]\n",
    "\n",
    "#                 opt.zero_grad()\n",
    "#                 scores_b = -model(xb)\n",
    "#                 if use_pairwise:\n",
    "#                     loss = pairwise_logistic_loss(scores_b, yb, margin=0.0)\n",
    "#                     if loss is None:\n",
    "#                         loss = bce(scores_b, yb)\n",
    "#                 else:\n",
    "#                     loss = bce(scores_b, yb)\n",
    "#                 loss.backward()\n",
    "#                 torch.nn.utils.clip_grad_norm_(\n",
    "#                     [model.raw_concentrations, model.raw_beta, model.raw_ace2], max_norm=10.0\n",
    "#                 )\n",
    "#                 opt.step()\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             scores_full = -model(X).cpu().numpy()\n",
    "#             auc = roc_auc_score(y, scores_full)\n",
    "#             rc = model.raw_concentrations.detach().cpu().numpy()\n",
    "#             rb = float(model.raw_beta.detach().cpu())\n",
    "#             ra = float(model.raw_ace2.detach().cpu())\n",
    "\n",
    "#         gm_c = float(10.0 ** (np.mean(rc)))\n",
    "#         gm_beta = float(np.exp(rb))\n",
    "#         check_date = col[len(check_prefix):]\n",
    "\n",
    "#         # parameters row\n",
    "#         row = {\n",
    "#             \"check_date\": check_date,\n",
    "#             \"raw_beta\": rb,\n",
    "#             \"raw_ace2\": ra,\n",
    "#             \"AUC\": auc,\n",
    "#         }\n",
    "#         # per-antibody raw_c columns\n",
    "#         for name, val in zip(ab_names, rc):\n",
    "#             row[f\"raw_c_{name}\"] = float(val)\n",
    "#         param_rows.append(row)\n",
    "\n",
    "#         # scores rows\n",
    "#         score_rows.extend(\n",
    "#             {\"check_date\": check_date, \"seq_index\": int(i), \"score\": float(s), \"y\": int(y[i])}\n",
    "#             for i, s in enumerate(scores_full)\n",
    "#         )\n",
    "\n",
    "#         print(f\"{col}: AUC={auc:.4f} raw_ace2={ra:.6g} gm_c={gm_c:.6g} gm_beta={gm_beta:.6g}\")\n",
    "\n",
    "#     params_df = pd.DataFrame(param_rows)\n",
    "#     scores_df = pd.DataFrame(score_rows)\n",
    "\n",
    "#     params_df.to_csv(params_csv, index=False)\n",
    "#     scores_df.to_csv(scores_csv, index=False)\n",
    "\n",
    "#     return params_df, scores_df\n",
    "\n",
    "# # Example:\n",
    "# params_df, scores_df = optimize_for_check_dates(df_out, seqs, params_csv=\"params_4ab.csv\", scores_csv=\"scores_4ab.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c95d8113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from math import ceil\n",
    "\n",
    "# # pairwise AUC surrogate\n",
    "# def pairwise_logistic_loss(scores: torch.Tensor, y: torch.Tensor, margin: float = 0.0, max_pairs: int = 200_000):\n",
    "#     pos = scores[y == 1]\n",
    "#     neg = scores[y == 0]\n",
    "#     if pos.numel() == 0 or neg.numel() == 0:\n",
    "#         return None\n",
    "#     num_pairs = pos.numel() * neg.numel()\n",
    "#     if num_pairs > max_pairs:\n",
    "#         k = max_pairs\n",
    "#         ip = torch.randint(0, pos.numel(), (k,), device=scores.device)\n",
    "#         ineg = torch.randint(0, neg.numel(), (k,), device=scores.device)\n",
    "#         diff = pos[ip] - neg[ineg]\n",
    "#     else:\n",
    "#         diff = pos[:, None] - neg[None, :]\n",
    "#     return F.softplus(-(diff - margin)).mean()\n",
    "\n",
    "# bce = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# def optimize_for_check_dates(\n",
    "#     df, seqs,\n",
    "#     check_prefix=\"y_\", lr=2e-1, weight_decay=1e-5,\n",
    "#     epochs=30, batch_size=1024, use_pairwise=True,\n",
    "#     device=None, dtype=torch.float64,\n",
    "#     params_csv=\"fitted_params.csv\", scores_csv=\"scores_by_date.csv\"\n",
    "# ):\n",
    "#     device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     check_cols = sorted([c for c in df.columns if c.startswith(check_prefix)])\n",
    "#     ab_names = list(KD_VECTORS.keys())\n",
    "\n",
    "#     X = torch.as_tensor(seqs, dtype=torch.long, device=device)\n",
    "\n",
    "#     param_rows = []\n",
    "#     score_rows = []\n",
    "\n",
    "#     for col in check_cols:\n",
    "#         y = df[col].to_numpy().astype(int)\n",
    "#         if y.sum() == 0 or y.sum() == y.size:\n",
    "#             print(f\"{col}: skipped (single class)\")\n",
    "#             continue\n",
    "\n",
    "#         Y = torch.as_tensor(y, dtype=dtype, device=device)\n",
    "\n",
    "#         model = EscapeMapTorch(\n",
    "#             rbm=RBM, kd_vectors=KD_VECTORS, ace2_vector=ACE2_KD_VECTOR,\n",
    "#             raw_concentrations=None, raw_ace2=None, raw_beta=None,\n",
    "#             total_beta=1.0, device=device, dtype=dtype\n",
    "#         ).to(device)\n",
    "\n",
    "#         opt = torch.optim.Adam(\n",
    "#             [model.raw_concentrations, model.raw_beta, model.raw_ace2],\n",
    "#             lr=lr, weight_decay=weight_decay\n",
    "#         )\n",
    "\n",
    "#         N = X.size(0)\n",
    "#         idx_all = torch.arange(N, device=device)\n",
    "\n",
    "#         for _ in range(epochs):\n",
    "#             perm = idx_all[torch.randperm(N, device=device)]\n",
    "#             for bi in range(ceil(N / batch_size)):\n",
    "#                 bidx = perm[bi * batch_size : (bi + 1) * batch_size]\n",
    "#                 xb = X[bidx]\n",
    "#                 yb = Y[bidx]\n",
    "\n",
    "#                 opt.zero_grad()\n",
    "#                 scores_b = -model(xb)\n",
    "#                 if use_pairwise:\n",
    "#                     loss = pairwise_logistic_loss(scores_b, yb, margin=0.0)\n",
    "#                     if loss is None:\n",
    "#                         loss = bce(scores_b, yb)\n",
    "#                 else:\n",
    "#                     loss = bce(scores_b, yb)\n",
    "#                 loss.backward()\n",
    "#                 torch.nn.utils.clip_grad_norm_(\n",
    "#                     [model.raw_concentrations, model.raw_beta, model.raw_ace2], max_norm=10.0\n",
    "#                 )\n",
    "#                 opt.step()\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             scores_full = -model(X).cpu().numpy()\n",
    "#             auc = roc_auc_score(y, scores_full)\n",
    "#             rc = model.raw_concentrations.detach().cpu().numpy()\n",
    "#             rb = float(model.raw_beta.detach().cpu())\n",
    "#             ra = float(model.raw_ace2.detach().cpu())\n",
    "\n",
    "#         gm_c = float(10.0 ** (np.mean(rc)))\n",
    "#         gm_beta = float(np.exp(rb))\n",
    "#         check_date = col[len(check_prefix):]\n",
    "\n",
    "#         # parameters row\n",
    "#         row = {\n",
    "#             \"check_date\": check_date,\n",
    "#             \"raw_beta\": rb,\n",
    "#             \"raw_ace2\": ra,\n",
    "#             \"AUC\": auc,\n",
    "#         }\n",
    "#         # per-antibody raw_c columns\n",
    "#         for name, val in zip(ab_names, rc):\n",
    "#             row[f\"raw_c_{name}\"] = float(val)\n",
    "#         param_rows.append(row)\n",
    "\n",
    "#         # scores rows\n",
    "#         score_rows.extend(\n",
    "#             {\"check_date\": check_date, \"seq_index\": int(i), \"score\": float(s), \"y\": int(y[i])}\n",
    "#             for i, s in enumerate(scores_full)\n",
    "#         )\n",
    "\n",
    "#         print(f\"{col}: AUC={auc:.4f} raw_ace2={ra:.6g} gm_c={gm_c:.6g} gm_beta={gm_beta:.6g}\")\n",
    "\n",
    "#     params_df = pd.DataFrame(param_rows)\n",
    "#     scores_df = pd.DataFrame(score_rows)\n",
    "\n",
    "#     params_df.to_csv(params_csv, index=False)\n",
    "#     scores_df.to_csv(scores_csv, index=False)\n",
    "\n",
    "#     return params_df, scores_df\n",
    "\n",
    "# # Example:\n",
    "# params_df, scores_df = optimize_for_check_dates(df_out, seqs, params_csv=\"params.csv\", scores_csv=\"scores.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f633f0",
   "metadata": {},
   "source": [
    "# one fit only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "560daf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from math import ceil\n",
    "\n",
    "# # pairwise AUC surrogate\n",
    "# def pairwise_logistic_loss(scores: torch.Tensor, y: torch.Tensor, margin: float = 0.0, max_pairs: int = 200_000):\n",
    "#     pos = scores[y == 1]\n",
    "#     neg = scores[y == 0]\n",
    "#     if pos.numel() == 0 or neg.numel() == 0:\n",
    "#         return None\n",
    "#     num_pairs = pos.numel() * neg.numel()\n",
    "#     if num_pairs > max_pairs:\n",
    "#         k = max_pairs\n",
    "#         ip = torch.randint(0, pos.numel(), (k,), device=scores.device)\n",
    "#         ineg = torch.randint(0, neg.numel(), (k,), device=scores.device)\n",
    "#         diff = pos[ip] - neg[ineg]\n",
    "#     else:\n",
    "#         diff = pos[:, None] - neg[None, :]\n",
    "#     return F.softplus(-(diff - margin)).mean()\n",
    "\n",
    "# bce = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# def optimize_for_check_dates(\n",
    "#     df, seqs,\n",
    "#     check_prefix=\"y_\", lr=2e-1, weight_decay=1e-5,\n",
    "#     epochs=30, batch_size=1024, use_pairwise=True,\n",
    "#     device=None, dtype=torch.float64,\n",
    "#     params_csv=\"fitted_params.csv\", scores_csv=\"scores_by_date.csv\"\n",
    "# ):\n",
    "#     device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     check_cols = sorted([c for c in df.columns if c.startswith(check_prefix)])\n",
    "#     ab_names = list(kd_vector_small.keys())\n",
    "\n",
    "#     X = torch.as_tensor(seqs, dtype=torch.long, device=device)\n",
    "\n",
    "#     param_rows = []\n",
    "#     score_rows = []\n",
    "\n",
    "#     for col in check_cols:\n",
    "#         y = Y_ALL\n",
    "#         if y.sum() == 0 or y.sum() == y.size:\n",
    "#             print(f\"{col}: skipped (single class)\")\n",
    "#             continue\n",
    "\n",
    "#         Y = torch.as_tensor(y, dtype=dtype, device=device)\n",
    "\n",
    "#         model = EscapeMapTorch(\n",
    "#             rbm=RBM, kd_vectors=kd_vector_small, ace2_vector=ACE2_KD_VECTOR,\n",
    "#             raw_concentrations=None, raw_ace2=None, raw_beta=None,\n",
    "#             total_beta=1.0, device=device, dtype=dtype\n",
    "#         ).to(device)\n",
    "\n",
    "#         opt = torch.optim.Adam(\n",
    "#             [model.raw_concentrations, model.raw_beta, model.raw_ace2],\n",
    "#             lr=lr, weight_decay=weight_decay\n",
    "#         )\n",
    "\n",
    "#         N = X.size(0)\n",
    "#         idx_all = torch.arange(N, device=device)\n",
    "\n",
    "#         for _ in range(epochs):\n",
    "#             perm = idx_all[torch.randperm(N, device=device)]\n",
    "#             for bi in range(ceil(N / batch_size)):\n",
    "#                 bidx = perm[bi * batch_size : (bi + 1) * batch_size]\n",
    "#                 xb = X[bidx]\n",
    "#                 yb = Y[bidx]\n",
    "\n",
    "#                 opt.zero_grad()\n",
    "#                 scores_b = -model(xb)\n",
    "#                 if use_pairwise:\n",
    "#                     loss = pairwise_logistic_loss(scores_b, yb, margin=0.0)\n",
    "#                     if loss is None:\n",
    "#                         loss = bce(scores_b, yb)\n",
    "#                 else:\n",
    "#                     loss = bce(scores_b, yb)\n",
    "#                 loss.backward()\n",
    "#                 torch.nn.utils.clip_grad_norm_(\n",
    "#                     [model.raw_concentrations, model.raw_beta, model.raw_ace2], max_norm=10.0\n",
    "#                 )\n",
    "#                 opt.step()\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             scores_full = -model(X).cpu().numpy()\n",
    "#             auc = roc_auc_score(y, scores_full)\n",
    "#             rc = model.raw_concentrations.detach().cpu().numpy()\n",
    "#             rb = float(model.raw_beta.detach().cpu())\n",
    "#             ra = float(model.raw_ace2.detach().cpu())\n",
    "\n",
    "#         gm_c = float(10.0 ** (np.mean(rc)))\n",
    "#         gm_beta = float(np.exp(rb))\n",
    "#         check_date = col[len(check_prefix):]\n",
    "\n",
    "#         # parameters row\n",
    "#         row = {\n",
    "#             \"check_date\": check_date,\n",
    "#             \"raw_beta\": rb,\n",
    "#             \"raw_ace2\": ra,\n",
    "#             \"AUC\": auc,\n",
    "#         }\n",
    "#         # per-antibody raw_c columns\n",
    "#         for name, val in zip(ab_names, rc):\n",
    "#             row[f\"raw_c_{name}\"] = float(val)\n",
    "#         param_rows.append(row)\n",
    "\n",
    "#         # scores rows\n",
    "#         score_rows.extend(\n",
    "#             {\"check_date\": check_date, \"seq_index\": int(i), \"score\": float(s), \"y\": int(y[i])}\n",
    "#             for i, s in enumerate(scores_full)\n",
    "#         )\n",
    "\n",
    "#         print(f\"{col}: AUC={auc:.4f} raw_ace2={ra:.6g} gm_c={gm_c:.6g} gm_beta={gm_beta:.6g}\")\n",
    "#         break\n",
    "\n",
    "#     params_df = pd.DataFrame(param_rows)\n",
    "#     scores_df = pd.DataFrame(score_rows)\n",
    "\n",
    "#     params_df.to_csv(params_csv, index=False)\n",
    "#     scores_df.to_csv(scores_csv, index=False)\n",
    "\n",
    "#     return params_df, scores_df\n",
    "\n",
    "# # Example:\n",
    "# params_df, scores_df = optimize_for_check_dates(df_out, seqs, params_csv=\"params_4ab_all.csv\", scores_csv=\"scores_4ab_all.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c759a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "model.raw_concentrations: Parameter containing:\n",
      "tensor([-12., -12., -12., -12.], dtype=torch.float64, requires_grad=True)\n",
      "y_2020-01: AUC=0.7216 raw_ace2=-16.3083 gm_c=1e-12 gm_beta=0.0700903\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from math import ceil\n",
    "\n",
    "# pairwise AUC surrogate\n",
    "def pairwise_logistic_loss(scores: torch.Tensor, y: torch.Tensor, margin: float = 0.0, max_pairs: int = 200_000):\n",
    "    pos = scores[y == 1]\n",
    "    neg = scores[y == 0]\n",
    "    if pos.numel() == 0 or neg.numel() == 0:\n",
    "        return None\n",
    "    num_pairs = pos.numel() * neg.numel()\n",
    "    if num_pairs > max_pairs:\n",
    "        k = max_pairs\n",
    "        ip = torch.randint(0, pos.numel(), (k,), device=scores.device)\n",
    "        ineg = torch.randint(0, neg.numel(), (k,), device=scores.device)\n",
    "        diff = pos[ip] - neg[ineg]\n",
    "    else:\n",
    "        diff = pos[:, None] - neg[None, :]\n",
    "    return F.softplus(-(diff - margin)).mean()\n",
    "\n",
    "bce = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "def optimize_for_check_dates(\n",
    "    df, seqs,\n",
    "    check_prefix=\"y_\", lr=2e-1, weight_decay=1e-5,\n",
    "    epochs=30, batch_size=1024, use_pairwise=True,\n",
    "    device=None, dtype=torch.float64,\n",
    "    params_csv=\"fitted_params.csv\", scores_csv=\"scores_by_date.csv\"\n",
    "):\n",
    "    device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    check_cols = sorted([c for c in df.columns if c.startswith(check_prefix)])\n",
    "    ab_names = list(kd_vector_small.keys())\n",
    "\n",
    "    X = torch.as_tensor(seqs, dtype=torch.long, device=device)\n",
    "\n",
    "    param_rows = []\n",
    "    score_rows = []\n",
    "\n",
    "    for col in check_cols:\n",
    "        y = Y_ALL\n",
    "        if y.sum() == 0 or y.sum() == y.size:\n",
    "            print(f\"{col}: skipped (single class)\")\n",
    "            continue\n",
    "\n",
    "        Y = torch.as_tensor(y, dtype=dtype, device=device)\n",
    "\n",
    "        model = EscapeMapTorch(\n",
    "            rbm=RBM, kd_vectors=kd_vector_small, ace2_vector=ACE2_KD_VECTOR,\n",
    "            raw_concentrations=None, raw_ace2=None, raw_beta=None,\n",
    "            total_beta=1.0, device=device, dtype=dtype\n",
    "        ).to(device)\n",
    "\n",
    " \n",
    "        \n",
    "        opt = torch.optim.Adam(\n",
    "            [model.raw_beta, model.raw_ace2],  # omit model.raw_concentrations\n",
    "            lr=lr, weight_decay=weight_decay\n",
    "        )\n",
    "\n",
    "        N = X.size(0)\n",
    "        idx_all = torch.arange(N, device=device)\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            perm = idx_all[torch.randperm(N, device=device)]\n",
    "            for bi in range(ceil(N / batch_size)):\n",
    "                bidx = perm[bi * batch_size : (bi + 1) * batch_size]\n",
    "                xb = X[bidx]\n",
    "                yb = Y[bidx]\n",
    "\n",
    "                opt.zero_grad()\n",
    "                scores_b = -model(xb)\n",
    "                if use_pairwise:\n",
    "                    loss = pairwise_logistic_loss(scores_b, yb, margin=0.0)\n",
    "                    if loss is None:\n",
    "                        loss = bce(scores_b, yb)\n",
    "                else:\n",
    "                    loss = bce(scores_b, yb)\n",
    "                loss.backward()\n",
    "                #gradient with respect to concentrations is put to 0\n",
    "                                # zero gradient for concentrations\n",
    "    \n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    [model.raw_concentrations, model.raw_beta, model.raw_ace2], max_norm=10.0\n",
    "                )\n",
    "                opt.step()\n",
    "                print('model.raw_concentrations:', model.raw_concentrations)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            scores_full = -model(X).cpu().numpy()\n",
    "            auc = roc_auc_score(y, scores_full)\n",
    "            rc = model.raw_concentrations.detach().cpu().numpy()\n",
    "            rb = float(model.raw_beta.detach().cpu())\n",
    "            ra = float(model.raw_ace2.detach().cpu())\n",
    "\n",
    "        gm_c = float(10.0 ** (np.mean(rc)))\n",
    "        gm_beta = float(np.exp(rb))\n",
    "        check_date = col[len(check_prefix):]\n",
    "\n",
    "        # parameters row\n",
    "        row = {\n",
    "            \"raw_beta\": rb,\n",
    "            \"raw_ace2\": ra,\n",
    "            \"AUC\": auc,\n",
    "        }\n",
    "        # per-antibody raw_c columns\n",
    "        for name, val in zip(ab_names, rc):\n",
    "            row[f\"raw_c_{name}\"] = float(val)\n",
    "        param_rows.append(row)\n",
    "\n",
    "        # scores rows\n",
    "        score_rows.extend(\n",
    "            {\"check_date\": check_date, \"seq_index\": int(i), \"score\": float(s), \"y\": int(y[i])}\n",
    "            for i, s in enumerate(scores_full)\n",
    "        )\n",
    "\n",
    "        print(f\"{col}: AUC={auc:.4f} raw_ace2={ra:.6g} gm_c={gm_c:.6g} gm_beta={gm_beta:.6g}\")\n",
    "        break\n",
    "\n",
    "    params_df = pd.DataFrame(param_rows)\n",
    "    scores_df = pd.DataFrame(score_rows)\n",
    "\n",
    "    params_df.to_csv(params_csv, index=False)\n",
    "    scores_df.to_csv(scores_csv, index=False)\n",
    "\n",
    "    return params_df, scores_df\n",
    "\n",
    "# Example:\n",
    "params_df, scores_df = optimize_for_check_dates(df_out, seqs, params_csv=\"params_noab_all.csv\", scores_csv=\"scores_noab_all.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33067282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_2020-01: AUC=0.5646 raw_ace2=-0.0502356 raw_beta=-3.36069\n",
      "y_2021-01: AUC=0.5349 raw_ace2=-0.0889108 raw_beta=-3.45023\n",
      "y_2022-01: AUC=0.5350 raw_ace2=-0.417715 raw_beta=-3.55572\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from math import ceil\n",
    "\n",
    "# -------- helpers --------\n",
    "\n",
    "def baseline_margin_loss(scores: torch.Tensor, y: torch.Tensor, baseline: torch.Tensor, margin: float = 0.0):\n",
    "    \"\"\"Enforce scores above WT for positives and below WT for negatives.\"\"\"\n",
    "    rel = scores - baseline  # relative to WT\n",
    "    loss = 0.0\n",
    "    if (y == 1).any():\n",
    "        pos = rel[y == 1]\n",
    "        loss = loss + F.softplus(-(pos - margin)).mean()\n",
    "    if (y == 0).any():\n",
    "        neg = rel[y == 0]\n",
    "        loss = loss + F.softplus(neg + margin).mean()\n",
    "    return loss\n",
    "\n",
    "def fit_date_with_wt_baseline(\n",
    "    seqs_np: np.ndarray,\n",
    "    y_np: np.ndarray,\n",
    "    kd_vectors: dict,\n",
    "    rbm,\n",
    "    ace2_vector,\n",
    "    epochs: int = 30,\n",
    "    batch_size: int = 1024,\n",
    "    lr: float = 2e-1,\n",
    "    weight_decay: float = 1e-5,\n",
    "    margin: float = 0.0,\n",
    "    device=None,\n",
    "    dtype=torch.float64,\n",
    "    freeze_conc: bool = True,\n",
    "):\n",
    "    \"\"\"Train EscapeMapTorch for one date with WT-baseline objective.\"\"\"\n",
    "    device = device or (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "    X = torch.as_tensor(seqs_np, dtype=torch.long, device=device)\n",
    "    Y = torch.as_tensor(y_np.astype(float), dtype=dtype, device=device)\n",
    "\n",
    "    model = EscapeMapTorch(\n",
    "        rbm=rbm, kd_vectors=kd_vectors, ace2_vector=ace2_vector,\n",
    "        total_beta=1.0, device=device, dtype=dtype\n",
    "    ).to(device)\n",
    "\n",
    "    # choose optimizer params\n",
    "    params = [model.raw_beta, model.raw_ace2] if freeze_conc else [model.raw_concentrations, model.raw_beta, model.raw_ace2]\n",
    "\n",
    "    opt = torch.optim.Adam(\n",
    "        [model.raw_concentrations, model.raw_beta, model.raw_ace2],\n",
    "        lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "    N = X.size(0); idx_all = torch.arange(N, device=device)\n",
    "    WT = torch.as_tensor(WT_SEQ, dtype=torch.long, device=device)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        perm = idx_all[torch.randperm(N, device=device)]\n",
    "        for bi in range(ceil(N / batch_size)):\n",
    "            bidx = perm[bi * batch_size : (bi + 1) * batch_size]\n",
    "            xb, yb = X[bidx], Y[bidx]\n",
    "\n",
    "            opt.zero_grad()\n",
    "            # recompute WT baseline under current params\n",
    "            wt_baseline = -model(WT)  # scalar\n",
    "            scores_b = -model(xb)     # [B]\n",
    "            loss = baseline_margin_loss(scores_b, yb, wt_baseline, margin=margin)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(params, max_norm=10.0)\n",
    "            opt.step()\n",
    "\n",
    "    # final evaluation with clipped scores\n",
    "    with torch.no_grad():\n",
    "        wt_baseline = -model(WT)\n",
    "        scores_full = -model(X)\n",
    "        rel_full = scores_full - wt_baseline\n",
    "        scores_clipped = torch.clamp_min(rel_full, 0.0).cpu().numpy()\n",
    "        auc = roc_auc_score(y_np, scores_clipped)\n",
    "        rc = model.raw_concentrations.detach().cpu().numpy()\n",
    "        rb = float(model.raw_beta.detach().cpu())\n",
    "        ra = float(model.raw_ace2.detach().cpu())\n",
    "\n",
    "    return {\n",
    "        \"scores_clipped\": scores_clipped,\n",
    "        \"raw_concentrations\": rc,\n",
    "        \"raw_beta\": rb,\n",
    "        \"raw_ace2\": ra,\n",
    "        \"AUC\": auc,\n",
    "    }\n",
    "\n",
    "# -------- main driver over dates, save 2 CSVs --------\n",
    "\n",
    "def optimize_for_check_dates_with_wt(\n",
    "    df: pd.DataFrame,\n",
    "    seqs: np.ndarray,\n",
    "    kd_vectors: dict,\n",
    "    rbm,\n",
    "    ace2_vector,\n",
    "    check_prefix: str = \"y_\",\n",
    "    epochs: int = 30,\n",
    "    batch_size: int = 1024,\n",
    "    lr: float = 2e-1,\n",
    "    weight_decay: float = 1e-5,\n",
    "    margin: float = 0.0,\n",
    "    device=None,\n",
    "    dtype=torch.float64,\n",
    "    params_csv: str = \"fitted_params.csv\",\n",
    "    scores_csv: str = \"scores_by_date.csv\",\n",
    "    freeze_conc: bool = True,\n",
    "):\n",
    "    device = device or (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "    check_cols = sorted([c for c in df.columns if c.startswith(check_prefix)])\n",
    "    ab_names = list(kd_vectors.keys())\n",
    "\n",
    "    param_rows = []\n",
    "    score_rows = []\n",
    "\n",
    "    for col in check_cols:\n",
    "        y = df[col].to_numpy().astype(int)\n",
    "        if y.sum() == 0 or y.sum() == y.size:\n",
    "            print(f\"{col}: skipped (single class)\")\n",
    "            continue\n",
    "\n",
    "        out = fit_date_with_wt_baseline(\n",
    "            seqs_np=seqs,\n",
    "            y_np=y,\n",
    "            kd_vectors=kd_vectors,\n",
    "            rbm=rbm,\n",
    "            ace2_vector=ace2_vector,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay,\n",
    "            margin=margin,\n",
    "            device=device,\n",
    "            dtype=dtype,\n",
    "            freeze_conc=freeze_conc,\n",
    "        )\n",
    "\n",
    "        check_date = col[len(check_prefix):]\n",
    "        # params row\n",
    "        row = {\n",
    "            \"check_date\": check_date,\n",
    "            \"raw_beta\": out[\"raw_beta\"],\n",
    "            \"raw_ace2\": out[\"raw_ace2\"],\n",
    "            \"AUC\": out[\"AUC\"],\n",
    "        }\n",
    "        for name, val in zip(ab_names, out[\"raw_concentrations\"]):\n",
    "            row[f\"raw_c_{name}\"] = float(val)\n",
    "        param_rows.append(row)\n",
    "\n",
    "        # scores rows\n",
    "        score_rows.extend(\n",
    "            {\"check_date\": check_date, \"seq_index\": int(i), \"score\": float(s), \"y\": int(y[i])}\n",
    "            for i, s in enumerate(out[\"scores_clipped\"])\n",
    "        )\n",
    "\n",
    "        print(f\"{col}: AUC={out['AUC']:.4f} raw_ace2={out['raw_ace2']:.6g} raw_beta={out['raw_beta']:.6g}\")\n",
    "\n",
    "    params_df = pd.DataFrame(param_rows)\n",
    "    scores_df = pd.DataFrame(score_rows)\n",
    "    params_df.to_csv(params_csv, index=False)\n",
    "    scores_df.to_csv(scores_csv, index=False)\n",
    "    return params_df, scores_df\n",
    "\n",
    "# -------- run example --------\n",
    "params_df, scores_df = optimize_for_check_dates_with_wt(\n",
    "    df=df_out,                  # DataFrame with y_* columns\n",
    "    seqs=seqs,                  # ndarray [N,L] int\n",
    "    kd_vectors=kd_vector_small, # or KD_VECTORS\n",
    "    rbm=RBM,\n",
    "    ace2_vector=ACE2_KD_VECTOR,\n",
    "    params_csv=\"params_pos_4ab_all.csv\",\n",
    "    scores_csv=\"scores_pos_4ab_all.csv\",\n",
    "    epochs=30, batch_size=1024, lr=2e-1, weight_decay=1e-5,\n",
    "    margin=0.0, freeze_conc=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a9b79eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.raw_beta: -0.19999999959827966\n",
      "model.raw_ace2: -9.800001329683786\n",
      "model.raw_beta: -0.39381662073214574\n",
      "model.raw_ace2: -9.600973365711658\n",
      "model.raw_beta: -0.576435298070775\n",
      "model.raw_ace2: -9.412372952888745\n",
      "model.raw_beta: -0.7502150356502543\n",
      "model.raw_ace2: -9.27784720658536\n",
      "model.raw_beta: -0.9101093962860822\n",
      "model.raw_ace2: -9.211237075712436\n",
      "model.raw_beta: -1.0557353721946743\n",
      "model.raw_ace2: -9.139316612447042\n",
      "model.raw_beta: -1.187834858498259\n",
      "model.raw_ace2: -9.113545144343853\n",
      "model.raw_beta: -1.306368912254779\n",
      "model.raw_ace2: -9.14178566210178\n",
      "model.raw_beta: -1.4130802578024997\n",
      "model.raw_ace2: -9.2052512937526\n",
      "model.raw_beta: -1.5091549220368636\n",
      "model.raw_ace2: -9.287907515933846\n",
      "model.raw_beta: -1.595649662132188\n",
      "model.raw_ace2: -9.393463866338065\n",
      "model.raw_beta: -1.6735845395949631\n",
      "model.raw_ace2: -9.517157427731116\n",
      "model.raw_beta: -1.7437985366527329\n",
      "model.raw_ace2: -9.654940722844255\n",
      "model.raw_beta: -1.8073031551864653\n",
      "model.raw_ace2: -9.808953038848067\n",
      "model.raw_beta: -1.864653418795616\n",
      "model.raw_ace2: -9.975167416629654\n",
      "model.raw_beta: -1.9164664958096407\n",
      "model.raw_ace2: -10.144624002939011\n",
      "model.raw_beta: -1.9632712480634724\n",
      "model.raw_ace2: -10.322861722348675\n",
      "model.raw_beta: -2.0056719956091826\n",
      "model.raw_ace2: -10.509646946599343\n",
      "model.raw_beta: -2.0440081530811205\n",
      "model.raw_ace2: -10.70154019425406\n",
      "model.raw_beta: -2.078705590649528\n",
      "model.raw_ace2: -10.902749216045224\n",
      "model.raw_beta: -2.1100808184704896\n",
      "model.raw_ace2: -11.108792713243396\n",
      "model.raw_beta: -2.1384524108979353\n",
      "model.raw_ace2: -11.319341518619654\n",
      "model.raw_beta: -2.1641195181767032\n",
      "model.raw_ace2: -11.533850829306362\n",
      "model.raw_beta: -2.1872403354083594\n",
      "model.raw_ace2: -11.752131239855855\n",
      "model.raw_beta: -2.208086583160618\n",
      "model.raw_ace2: -11.973159196319367\n",
      "model.raw_beta: -2.2269649538282517\n",
      "model.raw_ace2: -12.198179537235912\n",
      "model.raw_beta: -2.2440572401719354\n",
      "model.raw_ace2: -12.423950220810118\n",
      "model.raw_beta: -2.2592485822086994\n",
      "model.raw_ace2: -12.65511450885709\n",
      "model.raw_beta: -2.272935923267851\n",
      "model.raw_ace2: -12.887971687400638\n",
      "model.raw_beta: -2.2851587789293952\n",
      "model.raw_ace2: -13.119776568544482\n",
      "model.raw_beta: -2.2961134029185715\n",
      "model.raw_ace2: -13.352047753747316\n",
      "model.raw_beta: -2.3059270503123837\n",
      "model.raw_ace2: -13.584756758860555\n",
      "model.raw_beta: -2.3146581000446256\n",
      "model.raw_ace2: -13.817049509416462\n",
      "model.raw_beta: -2.3225232052536473\n",
      "model.raw_ace2: -14.047946017020994\n",
      "model.raw_beta: -2.3294956051989817\n",
      "model.raw_ace2: -14.276259306784942\n",
      "model.raw_beta: -2.3357120337904242\n",
      "model.raw_ace2: -14.50402853457983\n",
      "model.raw_beta: -2.341216875808857\n",
      "model.raw_ace2: -14.72823643564959\n",
      "model.raw_beta: -2.3459994262082633\n",
      "model.raw_ace2: -14.947956494045481\n",
      "model.raw_beta: -2.350203387839707\n",
      "model.raw_ace2: -15.162989527874625\n",
      "model.raw_beta: -2.353782254412656\n",
      "model.raw_ace2: -15.370101067537654\n",
      "model.raw_beta: -2.3568419863781234\n",
      "model.raw_ace2: -15.57008298284217\n",
      "model.raw_beta: -2.3595041608899927\n",
      "model.raw_ace2: -15.763332578844258\n",
      "model.raw_beta: -2.3617153093102887\n",
      "model.raw_ace2: -15.94793672115573\n",
      "model.raw_beta: -2.3636340893628187\n",
      "model.raw_ace2: -16.12536226950141\n",
      "model.raw_beta: -2.3652123739310826\n",
      "model.raw_ace2: -16.29365703472163\n",
      "model.raw_beta: -2.366422876789368\n",
      "model.raw_ace2: -16.453518229067388\n",
      "model.raw_beta: -2.3673313724469356\n",
      "model.raw_ace2: -16.604313636299487\n",
      "model.raw_beta: -2.367988531962849\n",
      "model.raw_ace2: -16.746022936685716\n",
      "model.raw_beta: -2.368366764107185\n",
      "model.raw_ace2: -16.878962897216294\n",
      "model.raw_beta: -2.368601759111164\n",
      "model.raw_ace2: -17.003119758061676\n",
      "model.raw_beta: -2.368636480430812\n",
      "model.raw_ace2: -17.118890641667168\n",
      "model.raw_beta: -2.3684334091977655\n",
      "model.raw_ace2: -17.226917407796783\n",
      "model.raw_beta: -2.3680117804522243\n",
      "model.raw_ace2: -17.327449294915127\n",
      "model.raw_beta: -2.367539107388794\n",
      "model.raw_ace2: -17.42039482596168\n",
      "model.raw_beta: -2.366978619050256\n",
      "model.raw_ace2: -17.506444172185237\n",
      "model.raw_beta: -2.3660715189107164\n",
      "model.raw_ace2: -17.586159861776306\n",
      "model.raw_beta: -2.36503833085802\n",
      "model.raw_ace2: -17.65973685726164\n",
      "model.raw_beta: -2.363901554726746\n",
      "model.raw_ace2: -17.727717566375432\n",
      "model.raw_beta: -2.362697454407437\n",
      "model.raw_ace2: -17.79016116310206\n",
      "model.raw_beta: -2.361286140914371\n",
      "model.raw_ace2: -17.848046316572372\n",
      "0.6872056786703601 -17.848046316572372 -2.361286140914371 0.10602924718532158 [ -3.19918279  -2.62195819  -3.54811318  -8.13782456  -8.41535727\n",
      "  -7.51788943  -9.54356127  -2.40612653  -9.41311055  -1.84031638\n",
      "  -8.40049957  -9.84893618 -10.89298334  -9.32631324 -10.9197343\n",
      " -10.74335276  -4.31934771  -9.71653971  -8.0814427   -7.77370675\n",
      "  -3.7076365   -7.93239375 -10.31315344  -9.03151815  -8.01589113\n",
      "  -2.81986552  -4.00346445  -6.84358631  -7.74397149]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from math import ceil\n",
    "\n",
    "# ===== EscapeMapTorch with learnable total_beta =====\n",
    "class EscapeMapTorch(nn.Module):\n",
    "    def __init__(\n",
    "        self, rbm, kd_vectors, ace2_vector,\n",
    "        raw_concentrations=None, raw_ace2=None, raw_beta=None,\n",
    "        total_beta=1.0, device=None, dtype=torch.float64\n",
    "    ):\n",
    "        super().__init__()\n",
    "        import math, numpy as np\n",
    "        self.device = device or (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "        self.dtype = dtype\n",
    "        self.kd_vectors = kd_vectors\n",
    "        self.ace2_vector = {\"ace2\": ace2_vector}\n",
    "        self.rbm = rbm\n",
    "        self.ln10 = math.log(10.0)\n",
    "\n",
    "        A = len(kd_vectors)\n",
    "        rc = torch.full((A,), -10.0, dtype=dtype, device=self.device) if raw_concentrations is None \\\n",
    "             else torch.as_tensor(raw_concentrations, dtype=dtype, device=self.device)\n",
    "        self.raw_concentrations = nn.Parameter(rc)\n",
    "\n",
    "        ra = -10.0 if raw_ace2 is None else float(raw_ace2)\n",
    "        rb =  0.0 if raw_beta is None else float(raw_beta)\n",
    "        self.raw_ace2 = nn.Parameter(torch.tensor(ra, dtype=dtype, device=self.device))\n",
    "        self.raw_beta = nn.Parameter(torch.tensor(rb, dtype=dtype, device=self.device))\n",
    "\n",
    "        # NEW: learnable total_beta (positive)\n",
    "        self.raw_total_beta = nn.Parameter(torch.tensor(float(np.log(total_beta)), dtype=dtype, device=self.device))\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_int16_c_contig(x):\n",
    "        import numpy as np\n",
    "        if x.dtype != np.int16:\n",
    "            x = x.astype(np.int16, copy=False)\n",
    "        return np.ascontiguousarray(x)\n",
    "\n",
    "    def _get_Kd_batch_numpy(self, seqs_np, vectors, log10=True):\n",
    "        import numpy as np\n",
    "        try:\n",
    "            out = get_Kd(seqs_np, vectors, log10=log10)\n",
    "        except Exception:\n",
    "            out = np.stack([get_Kd(x, vectors, log10=log10) for x in seqs_np], axis=0)\n",
    "        return np.asarray(out)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _rbm_free_energy(self, seqs_np):\n",
    "        fe = self.rbm.free_energy(seqs_np)\n",
    "        return torch.as_tensor(fe, dtype=self.dtype, device=self.device)\n",
    "\n",
    "    def forward(self, s):\n",
    "        import numpy as np\n",
    "        s_np = np.asarray(s)\n",
    "        single = (s_np.ndim == 1)\n",
    "        seqs_np = s_np[None, :] if single else s_np\n",
    "        seqs_np = self._to_int16_c_contig(seqs_np)\n",
    "\n",
    "        beta = torch.exp(self.raw_beta)\n",
    "        total_beta = torch.exp(self.raw_total_beta)   # positive\n",
    "        ln10 = self.ln10\n",
    "\n",
    "        kds_np = self._get_Kd_batch_numpy(seqs_np, self.kd_vectors, log10=True) * ln10\n",
    "        kds = torch.as_tensor(np.squeeze(kds_np), dtype=self.dtype, device=self.device)\n",
    "\n",
    "        kdace2_np = self._get_Kd_batch_numpy(seqs_np, self.ace2_vector, log10=True) * ln10\n",
    "        kdace2_np = np.clip(np.squeeze(kdace2_np), -15.0, -5.0)\n",
    "        kdace2 = torch.as_tensor(kdace2_np, dtype=self.dtype, device=self.device)\n",
    "\n",
    "        conc = self.raw_concentrations * ln10\n",
    "        def softplus_stable(x):\n",
    "            out = torch.empty_like(x)\n",
    "            m = x > 1\n",
    "            out[m] = x[m] + torch.log1p(torch.exp(-x[m]))\n",
    "            out[~m] = torch.log1p(torch.exp(x[~m]))\n",
    "            return out\n",
    "\n",
    "        energy = softplus_stable(-kds + conc).sum(dim=-1)\n",
    "        energy = energy + softplus_stable(kdace2 - self.raw_ace2)\n",
    "\n",
    "        fe = self._rbm_free_energy(seqs_np)\n",
    "        energy = (energy + beta * fe) * total_beta\n",
    "        return energy[0] if single else energy\n",
    "\n",
    "# ===== Training with BCE on sigmoid(score - WT) =====\n",
    "def fit_escape_map_sigmoid_diff(\n",
    "    seqs, y, rbm, kd_vectors, ace2_vector,\n",
    "    epochs=50, batch_size=512, lr=2e-1, weight_decay=1e-5,\n",
    "    device=None, dtype=torch.float64\n",
    "):\n",
    "    device = device or (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "    X = torch.as_tensor(seqs, dtype=torch.long, device=device)\n",
    "    Y = torch.as_tensor(y.astype(float), dtype=dtype, device=device)\n",
    "\n",
    "    model = EscapeMapTorch(\n",
    "        rbm=rbm, kd_vectors=kd_vectors, ace2_vector=ace2_vector,\n",
    "        total_beta=1.0, device=device, dtype=dtype\n",
    "    ).to(device)\n",
    "\n",
    "    # Optimizer: now include raw_concentrations\n",
    "    params = [model.raw_concentrations, model.raw_beta, model.raw_ace2, model.raw_total_beta]\n",
    "    opt = torch.optim.Adam(params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Class imbalance handling\n",
    "    pos = float(Y.sum().item()); neg = float(Y.numel() - pos)\n",
    "    pos_weight = torch.tensor(neg / max(pos, 1.0), dtype=dtype, device=device)\n",
    "    bce = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    N = X.size(0); idx_all = torch.arange(N, device=device)\n",
    "    WT = torch.as_tensor(WT_SEQ, dtype=torch.long, device=device)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        perm = idx_all[torch.randperm(N, device=device)]\n",
    "        for bi in range(ceil(N / batch_size)):\n",
    "            bidx = perm[bi * batch_size : (bi + 1) * batch_size]\n",
    "            xb, yb = X[bidx], Y[bidx]\n",
    "\n",
    "            opt.zero_grad()\n",
    "            wt_score = -model(WT)               # scalar\n",
    "            scores_b = -model(xb)               # [B]\n",
    "            logits = scores_b - wt_score        # classification margin vs WT\n",
    "            loss = bce(logits, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(params, max_norm=10.0)\n",
    "            opt.step()\n",
    "            print('model.raw_beta:', model.raw_beta.item())\n",
    "            print('model.raw_ace2:', model.raw_ace2.item())\n",
    "\n",
    "    # Final evaluation\n",
    "    with torch.no_grad():\n",
    "        wt_score = -model(WT)\n",
    "        scores = -model(X)\n",
    "        logits_full = scores - wt_score\n",
    "        prob = torch.sigmoid(logits_full).cpu().numpy()\n",
    "        auc = roc_auc_score(y, prob)\n",
    "        rc = model.raw_concentrations.detach().cpu().numpy()\n",
    "        rb = float(model.raw_beta.detach().cpu())\n",
    "        ra = float(model.raw_ace2.detach().cpu())\n",
    "        tb = float(torch.exp(model.raw_total_beta).detach().cpu())  # total_beta in linear scale\n",
    "\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"prob\": prob,\n",
    "        \"AUC\": auc,\n",
    "        \"raw_concentrations\": rc,\n",
    "        \"raw_beta\": rb,\n",
    "        \"raw_ace2\": ra,\n",
    "        \"total_beta\": tb,\n",
    "    }\n",
    "\n",
    "\n",
    "out = fit_escape_map_sigmoid_diff(\n",
    "    seqs=seqs, y=Y_ALL, rbm=RBM, kd_vectors=KD_VECTORS, ace2_vector=ACE2_KD_VECTOR,\n",
    "    epochs=30, batch_size=1024, lr=2e-1, weight_decay=1e-5,\n",
    ")\n",
    "print(out[\"AUC\"], out[\"raw_ace2\"], out[\"raw_beta\"], out[\"total_beta\"], out[\"raw_concentrations\"])\n",
    "# --- save scores ---\n",
    "scores_df = pd.DataFrame({\n",
    "    \"seq_index\": np.arange(len(seqs)),\n",
    "    \"y\": Y_ALL,\n",
    "    \"score\": out[\"prob\"]\n",
    "})\n",
    "scores_df.to_csv(\"scores_sigmoid_diff.csv\", index=False)\n",
    "\n",
    "# --- save parameters ---\n",
    "ab_names = list(KD_VECTORS.keys())\n",
    "param_row = {\n",
    "    \"AUC\": out[\"AUC\"],\n",
    "    \"raw_beta\": out[\"raw_beta\"],\n",
    "    \"raw_ace2\": out[\"raw_ace2\"],\n",
    "    \"total_beta\": out[\"total_beta\"],\n",
    "}\n",
    "for name, val in zip(ab_names, out[\"raw_concentrations\"]):\n",
    "    param_row[f\"raw_c_{name}\"] = float(val)\n",
    "\n",
    "params_df = pd.DataFrame([param_row])\n",
    "params_df.to_csv(\"params_sigmoid_diff.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# out = fit_escape_map_sigmoid_diff(\n",
    "#     seqs=seqs, y=Y_ALL, rbm=RBM, kd_vectors=kd_vector_small, ace2_vector=ACE2_KD_VECTOR,\n",
    "#     epochs=30, batch_size=2048, lr=2e-1, weight_decay=1e-5,\n",
    "# )\n",
    "# print(out[\"AUC\"], out[\"raw_ace2\"], out[\"raw_beta\"], out[\"total_beta\"], out[\"raw_concentrations\"])\n",
    "# # --- save scores ---\n",
    "# scores_df = pd.DataFrame({\n",
    "#     \"seq_index\": np.arange(len(seqs)),\n",
    "#     \"y\": Y_ALL,\n",
    "#     \"score\": out[\"prob\"]\n",
    "# })\n",
    "# scores_df.to_csv(\"scores_sigmoid_diff_4ab.csv\", index=False)\n",
    "\n",
    "# # --- save parameters ---\n",
    "# ab_names = list(kd_vector_small.keys())\n",
    "# param_row = {\n",
    "#     \"AUC\": out[\"AUC\"],\n",
    "#     \"raw_beta\": out[\"raw_beta\"],\n",
    "#     \"raw_ace2\": out[\"raw_ace2\"],\n",
    "#     \"total_beta\": out[\"total_beta\"],\n",
    "# }\n",
    "# for name, val in zip(ab_names, out[\"raw_concentrations\"]):\n",
    "#     param_row[f\"raw_c_{name}\"] = float(val)\n",
    "\n",
    "# params_df = pd.DataFrame([param_row])\n",
    "# params_df.to_csv(\"params_sigmoid_diff_4ab.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef25d2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from math import ceil\n",
    "\n",
    "# # pairwise AUC surrogate\n",
    "# def pairwise_logistic_loss(scores: torch.Tensor, y: torch.Tensor, margin: float = 0.0, max_pairs: int = 200_000):\n",
    "#     pos = scores[y == 1]\n",
    "#     neg = scores[y == 0]\n",
    "#     if pos.numel() == 0 or neg.numel() == 0:\n",
    "#         return None\n",
    "#     num_pairs = pos.numel() * neg.numel()\n",
    "#     if num_pairs > max_pairs:\n",
    "#         k = max_pairs\n",
    "#         ip = torch.randint(0, pos.numel(), (k,), device=scores.device)\n",
    "#         ineg = torch.randint(0, neg.numel(), (k,), device=scores.device)\n",
    "#         diff = pos[ip] - neg[ineg]\n",
    "#     else:\n",
    "#         diff = pos[:, None] - neg[None, :]\n",
    "#     return F.softplus(-(diff - margin)).mean()\n",
    "\n",
    "# bce = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# def optimize_for_check_dates(\n",
    "#     df, seqs,\n",
    "#     check_prefix=\"y_\", lr=2e-1, weight_decay=1e-5,\n",
    "#     epochs=30, batch_size=1024, use_pairwise=True,\n",
    "#     device=None, dtype=torch.float64,\n",
    "#     params_csv=\"fitted_params.csv\", scores_csv=\"scores_by_date.csv\"\n",
    "# ):\n",
    "#     device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     check_cols = sorted([c for c in df.columns if c.startswith(check_prefix)])\n",
    "#     ab_names = list(KD_VECTORS.keys())\n",
    "\n",
    "#     X = torch.as_tensor(seqs, dtype=torch.long, device=device)\n",
    "\n",
    "#     param_rows = []\n",
    "#     score_rows = []\n",
    "\n",
    "#     for col in check_cols:\n",
    "#         y = Y_ALL\n",
    "#         if y.sum() == 0 or y.sum() == y.size:\n",
    "#             print(f\"{col}: skipped (single class)\")\n",
    "#             continue\n",
    "\n",
    "#         Y = torch.as_tensor(y, dtype=dtype, device=device)\n",
    "\n",
    "#         model = EscapeMapTorch(\n",
    "#             rbm=RBM, kd_vectors=KD_VECTORS, ace2_vector=ACE2_KD_VECTOR,\n",
    "#             raw_concentrations=None, raw_ace2=None, raw_beta=None,\n",
    "#             total_beta=1.0, device=device, dtype=dtype\n",
    "#         ).to(device)\n",
    "\n",
    "#         opt = torch.optim.Adam(\n",
    "#             [model.raw_concentrations, model.raw_beta, model.raw_ace2],\n",
    "#             lr=lr, weight_decay=weight_decay\n",
    "#         )\n",
    "\n",
    "#         N = X.size(0)\n",
    "#         idx_all = torch.arange(N, device=device)\n",
    "\n",
    "#         for _ in range(epochs):\n",
    "#             perm = idx_all[torch.randperm(N, device=device)]\n",
    "#             for bi in range(ceil(N / batch_size)):\n",
    "#                 bidx = perm[bi * batch_size : (bi + 1) * batch_size]\n",
    "#                 xb = X[bidx]\n",
    "#                 yb = Y[bidx]\n",
    "\n",
    "#                 opt.zero_grad()\n",
    "#                 scores_b = -model(xb)\n",
    "#                 if use_pairwise:\n",
    "#                     loss = pairwise_logistic_loss(scores_b, yb, margin=0.0)\n",
    "#                     if loss is None:\n",
    "#                         loss = bce(scores_b, yb)\n",
    "#                 else:\n",
    "#                     loss = bce(scores_b, yb)\n",
    "#                 loss.backward()\n",
    "#                 torch.nn.utils.clip_grad_norm_(\n",
    "#                     [model.raw_concentrations, model.raw_beta, model.raw_ace2], max_norm=10.0\n",
    "#                 )\n",
    "#                 opt.step()\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             scores_full = -model(X).cpu().numpy()\n",
    "#             auc = roc_auc_score(y, scores_full)\n",
    "#             rc = model.raw_concentrations.detach().cpu().numpy()\n",
    "#             rb = float(model.raw_beta.detach().cpu())\n",
    "#             ra = float(model.raw_ace2.detach().cpu())\n",
    "\n",
    "#         gm_c = float(10.0 ** (np.mean(rc)))\n",
    "#         gm_beta = float(np.exp(rb))\n",
    "#         check_date = col[len(check_prefix):]\n",
    "\n",
    "#         # parameters row\n",
    "#         row = {\n",
    "#             \"check_date\": check_date,\n",
    "#             \"raw_beta\": rb,\n",
    "#             \"raw_ace2\": ra,\n",
    "#             \"AUC\": auc,\n",
    "#         }\n",
    "#         # per-antibody raw_c columns\n",
    "#         for name, val in zip(ab_names, rc):\n",
    "#             row[f\"raw_c_{name}\"] = float(val)\n",
    "#         param_rows.append(row)\n",
    "\n",
    "#         # scores rows\n",
    "#         score_rows.extend(\n",
    "#             {\"check_date\": check_date, \"seq_index\": int(i), \"score\": float(s), \"y\": int(y[i])}\n",
    "#             for i, s in enumerate(scores_full)\n",
    "#         )\n",
    "\n",
    "#         print(f\"{col}: AUC={auc:.4f} raw_ace2={ra:.6g} gm_c={gm_c:.6g} gm_beta={gm_beta:.6g}\")\n",
    "#         break\n",
    "\n",
    "#     params_df = pd.DataFrame(param_rows)\n",
    "#     scores_df = pd.DataFrame(score_rows)\n",
    "\n",
    "#     params_df.to_csv(params_csv, index=False)\n",
    "#     scores_df.to_csv(scores_csv, index=False)\n",
    "\n",
    "#     return params_df, scores_df\n",
    "\n",
    "# # Example:\n",
    "# params_df, scores_df = optimize_for_check_dates(df_out, seqs, params_csv=\"params_all.csv\", scores_csv=\"scores_all.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
